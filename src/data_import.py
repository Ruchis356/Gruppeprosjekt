# Possible improvement: Use lists to append data before making a dataframe rather than "appending" to an empty dataframe



import requests
import pandas as pd
import numpy as np

#A class to import and handle environmental data (weather and air quality)
class RawData:

    #Initialize the EnvironmentalData class
    def __init__(self):
        self.df = None

    # ------------------------------------------
    # WEATHER DATA - IMPORT
    # ------------------------------------------

    #Fetch weather data from the Frost API 
    def get_met(self, station_id, elements, time_range, resolution):

        """
        Fetch weather data from the Frost API.

        Args:
            station_id (str): The ID of the weather station.
            elements (str): The measurements to include.
            time_range (str): The time range for the data.
            resolution (str): The granularity of the data.

        Returns:
            pd.DataFrame: A DataFrame containing the weather data.
                Returns None if an error occurs.
        """        

        try:

            # The following block of code (if statements) was mainly generated by AI, lightly reworked
            # - Purpose: Raise an error if the given arguments are invalid
            # - AI tool: DeepSeek

            # Validate the arguments given to the function
            if not station_id or not elements or not time_range or not resolution:
                raise ValueError("All input parameters must be provided.")

            if not isinstance(station_id, str):
                raise ValueError("station_id must be a string.")

            if not isinstance(elements, str):
                raise ValueError("elements must be a string.")

            if not isinstance(time_range, str):
                raise ValueError("time_range must be a string.")

            if not isinstance(resolution, str):
                raise ValueError("resolution must be a string.")

            # If the arguments are valid, proceed with the API request

            # Client ID to access data from Frost API
            client_id = 'd933f861-70f3-4d0f-adc6-b1edb5978a9e'

            # The following block of code (the four variable lines) was generated with the assistance of AI
            # - Purpose: Correctly call upon the four variables in this function
            # - AI tool: DeepSeek

            # Define endpoints and parameters
            endpoint = 'https://frost.met.no/observations/v0.jsonld'
            parameters = {
                'sources': station_id,  # Station ID 
                'elements': elements,  # Requestion various types of weather data
                'referencetime': time_range,  # Limiting the time frame for the data request
                'timeresolutions': resolution,  # Set the resolution(granularity) of the data
                'levels': 'default',
                'timeoffsets': 'default', # Selects the best timeiffset value available, first PT6H then PT0H
                'qualities': '0,1,2,3,4' # Only import data of a high enough quailty as explained here: https://frost.met.no/dataclarifications.html#quality-code
            }

            # Send an HTTP GET-request
            response = requests.get(endpoint, params=parameters, auth=(client_id, ''))

            # Extract JSON-data
            json_data = response.json()

            # Check if the request was succesfull, and exit if not
            if response.status_code == 200:
                data = json_data['data']
                print('Data collected from frost.met.no!')
            else:
                print('Error! Statuscode:', response.status_code)
                print('Message:', json_data['error']['message'])
                print('Cause:', json_data['error']['reason'])
                return None

            # Create and set up the dataframe
            data_list = []
            for obs in data:
                if isinstance(obs['observations'], list):
                    for observation in obs['observations']:
                        row = observation.copy()
                        row['referenceTime'] = obs['referenceTime']
                        data_list.append(row)
                else:
                    row = obs['observations'].copy()
                    row['referenceTime'] = obs['referenceTime']
                    data_list.append(row)
            df = pd.DataFrame(data_list)

            # Remove uneeded collumns  
            columns_to_drop = ["level", "timeResolution", "timeSeriesId", "elementId", "performanceCategory", "exposureCategory", "qualityCode"]
            df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

            # Remove time portion from 'referenceTime' (keep only 'YYYY-MM-DD')
            df["referenceTime"] = df["referenceTime"].str.split("T").str[0]

            print('There are ', df.shape[0]-1, 'lines of data in this dataframe.\n')
            self.df = df

            # The following block (if) was generated with the assistance of AI
            # Purpose: AI generated the df.drop_duplicates line and suggested how to use df.pivot
            # AI tool: DeepSeek            

            if not df.empty:
                # Check rows for duplicate/multiple values and only keeping one
                df = df.drop_duplicates(subset=['referenceTime', 'unit'], keep='first')
                
                # Pivot to wide format
                pivoted_df = df.pivot(
                    index='referenceTime',
                    columns='unit',
                    values='value'
                ).reset_index()
                
                # Clean up column names
                pivoted_df.columns.name = None
                df = pivoted_df.rename(columns={
                    'degC': 'temperature (C)',
                    'mm': 'precipitation (mm)',
                    'm/s': 'wind_speed (m/s)',
                    'referenceTime': 'Date'
                })

            #Returns dataframe upon request
            return(df)
        
        # The following block was generated with the assistance of AI
        # Purpose: Including more specific errors; Connection, timeout, and HTTP
        # AI tool: DeepSeek

        # Return an error code if fetching the weather data fails
        except ValueError as e:
            print(f"Invalid input: {e}")
            return None
        except requests.exceptions.ConnectionError:
            print("Error: Failed to connect to the Frost API. Check your internet connection.")
            return None
        except requests.exceptions.Timeout:
            print("Error: The request to the Frost API timed out. Please try again later.")
            return None
        except requests.exceptions.HTTPError as e:
            print(f"Error: The Frost API returned an HTTP error: {e}")
            return None
        except Exception as e:
            print(f"An unexpected error has occured: {e}")
            return None


    # ------------------------------------------
    # AIR QUALITY DATA - IMPORT
    # ------------------------------------------

    # Fetch air quality data by Nilu from a CSV file
    def get_nilu(self, threshold, file_path): 

        """
        Args:
            file_path (str): Path to the CSV file.
            threshold (float): Threshold for coverage values.

        Returns:
            pd.DataFrame: A DataFrame containing the air quality data.
        """

        # The following block was optimised for lower resource use with the assistance of AI
        # Purpose: reduce runtime and improve of the code when reading the csv file
        # AI tool: DeepSeek

        try:
            # parsing csv file
            df = pd.read_csv(
                file_path,
                skiprows=3,
                sep=';',
                on_bad_lines='skip',
                encoding='utf-8',
                parse_dates=['Tid'],
                date_format='%d.%m.%Y %H:%M',
                na_values='',  # Replace empty strings with NaN
                decimal=',',   # Handle comma decimals correctly
                dtype={col: 'float64' for col in pd.read_csv(file_path, nrows=1, skiprows=3, sep=';').columns 
                    if col != 'Tid'}  # Pre-specify dtypes
            )
            
            # The following two blocks are an improvement made based on a suggestion from AI
            # Purpose: Entirely replace a function in 'data_handling' that dealt with coverage by utilising and removing the overage columns within this function
            # AI tool: DeepSeek
            
            # Process coverage columns
            coverage_cols = df.columns[df.columns.str.contains('Dekning', case=False)].tolist()            
            for cov_col in coverage_cols:
                meas_col = df.columns[df.columns.get_loc(cov_col) - 1] # Get corresponding measurement column
                df.loc[df[cov_col] < threshold, meas_col] = np.nan # Set measurements to NaN where coverage is below threshold
            df = df.drop(columns=coverage_cols) # Remove all coverage columns after processing
            
            # Simplify remaining column names
            new_cols = {'Tid': 'Date'}
            for col in df.columns:
                if col != 'Tid':
                    # Extract pollutant type (NO, NO2, etc.) to create new column names
                    pollutant = col.split()[1] 
                    new_cols[col] = pollutant 
            df = df.rename(columns=new_cols)

            # Print diagnostics
            print('Data collected from nilu.com!')
            print(f'There are {df.shape[0]} lines of data in this dataframe.\n')
            self.df = df
            
            # Return the dataframe
            return df
        
        # The following block was generated with the assistance of AI
        # Purpose: Including more specific errors; FileNotFound, ParserError
        # AI tool: DeepSeek

        # Return an error code if reading the csv file failed
        except FileNotFoundError:
            print(f"Error: The file '{file_path}' was not found. Check the file path.")
            return None
        except pd.errors.ParserError:
            print("Error: Could not read the csv file. Check the formatting and encoding.")
            return None
        except Exception as e:
            print(f"An unexpected error has occured: {e}")
            return None

