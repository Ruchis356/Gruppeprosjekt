import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from datetime import timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import mutual_info_regression
import logging # The use of logging was suggested by AI (DeepSeek)
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)


class WeatherAnalyser:
    ''' A class for analysing and predicting weather and pollutions using historical data and the weather forecast.'''

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def load_and_merge_data(self, df_weather, df_quality, weather_vars, pollutant_vars, mode='train'):
        
        """
        Merges dataframes.
        
        Args:
            df_weather: DataFrame with columns ['Date', 'temperature (C)', ...]
            df_quality: DataFrame with columns ['Date', 'PM10', ...]
            weather_vars: The columns to be found in df_weather
            pllutant_vars: The columns to be found in df_quality
            mode: 'train' or 'test' (to handle column name suffixes)

        Returns:
            Merged DataFrame with cleaned data and DayOfYear column
        """

        # The following block of code was generated by AI
            # Purpose: graceful handling of missing columns in the dataframes
            # AI Tool: DeepSeek

        # Check available columns in both datasets and log any missing
        available_weather = [col for col in weather_vars if col in df_weather.columns]
        available_pollutants = [col for col in pollutant_vars if col in df_quality.columns]
        missing_weather = set(weather_vars) - set(available_weather)
        missing_pollutants = set(pollutant_vars) - set(available_pollutants)

        # Handle merging of test data
        if mode == 'test':
            df_quality = df_quality.rename(columns={c: c.replace('_test', '') for c in df_quality.columns})
        
        # Ensure we have at least one column from each dataset
        if not available_weather:
            raise ValueError("No weather columns available - check input data")
        if not available_pollutants:
            self.logger.warning("No pollutant columns available - proceeding with weather data only")

        # Merge with available columns
        merged = pd.merge(
            df_weather[['Date'] + available_weather],
            df_quality[['Date'] + available_pollutants],
            on='Date',
            how='inner'
        )
        
        # Add day of year 
        merged['DayOfYear'] = merged['Date'].dt.dayofyear
        
        if missing_weather:
            self.logger.info(f"\nSkipping missing weather columns: {missing_weather}")
        if missing_pollutants:
            self.logger.info(f"\nSkipping missing pollutant columns: {missing_pollutants}")

        self.logger.info(
            f"\nMerged data shape: {merged.shape}, "
            f"\nAvailable weather: {available_weather}, "
            f"\nAvailable pollutants: {available_pollutants}\n"
        )

        merged['DayOfYear_sin'] = np.sin(2 * np.pi * merged['DayOfYear']/365)
        merged['DayOfYear_cos'] = np.cos(2 * np.pi * merged['DayOfYear']/365)
        merged['Weekend'] = merged['Date'].dt.weekday >= 5  # Saturday/Sunday
        merged['Season'] = merged['Date'].dt.month % 12 // 3 + 1  # 1-4
        merged['weekend_effect'] = merged['Weekend'] * merged['DayOfYear_sin']
        merged['Date'] = pd.to_datetime(merged['Date'])
        merged = merged.sort_values('Date') 
                
        if 'temperature (C)' in weather_vars and 'wind_speed (m/s)' in weather_vars:
            merged['temp_wind_interaction'] = merged['temperature (C)'] * merged['wind_speed (m/s)']
            
        if 'temperature (C)' in weather_vars and 'precipitation (mm)' in weather_vars:
            merged['temp_precip_interaction'] = merged['temperature (C)'] * merged['precipitation (mm)']

        if 'precipitation (mm)' in merged.columns and 'wind_speed (m/s)' in merged.columns:
            merged['precip_wind_interaction'] = merged['precipitation (mm)'] * merged['wind_speed (m/s)']

        interactions = []
        # Create all possible interaction terms
        for i, var1 in enumerate(weather_vars):
            for var2 in weather_vars[i+1:]:
                col_name = f"{var1.split()[0]}_{var2.split()[0]}_interaction"
                merged[col_name] = merged[var1] * merged[var2]
                interactions.append(col_name)

        # Rolling features (only if we have enough history)
        for var in weather_vars:
            if var in merged.columns:
                base_name = var.split()[0]  # 'temperature' -> 'rolling_temperature_7'
                merged[f'rolling_{base_name}_7'] = merged[var].rolling(7, min_periods=1).mean()

        for lag in [1, 3, 7, 30]:  # 1-day, weekly, monthly lags
            for pollutant in pollutant_vars:
                if pollutant in merged.columns:
                    merged[f'{pollutant}_lag_{lag}'] = merged[pollutant].shift(lag)

        for lag in [1, 2, 3]:  # Start with just lag_1 if you prefer
            for weather_var in ['temperature (C)', 'wind_speed (m/s)', 'precipitation (mm)']:
                if weather_var in merged.columns:
                    merged[f'{weather_var}_lag_{lag}'] = merged[weather_var].shift(lag)

        for pollutant in pollutant_vars:
            if pollutant in merged.columns:
                merged[f'rolling_{pollutant}_7'] = merged[pollutant].rolling(7, min_periods=1).mean()
                merged[f'{pollutant}_lag_14'] = merged[pollutant].shift(14)

        for pollutant in pollutant_vars:
            if pollutant in merged:
                merged[pollutant] = merged[pollutant].interpolate(limit=3)

        for pollutant in pollutant_vars:
            if pollutant in merged:
                na_count_before = merged[pollutant].isna().sum()
                merged[pollutant] = merged[pollutant].interpolate(limit=3)
                na_count_after = merged[pollutant].isna().sum()
                if na_count_after < na_count_before:
                    self.logger.info(f"Interpolated {na_count_before - na_count_after} missing values in {pollutant}")

        merged['spike_indicator'] = (merged['PM10_lag_1'] < 20) & (merged['wind_speed (m/s)'] < 3)
        merged['wind_speed_change'] = merged['wind_speed (m/s)'] - merged['wind_speed (m/s)_lag_1']
        merged = merged.dropna(subset=weather_vars)

        return merged
    
    def safe_fit(self, model, X, y):
        """
        Safely fits a model by dropping rows where the target (y) has NaN values.
        
        Args:
            model: A scikit-learn compatible model with a `fit` method.
            X (pd.DataFrame): Feature matrix.
            y (pd.Series): Target variable.
            
        Returns:
            The fitted model.
        """
        valid_mask = y.notna()
        return model.fit(X[valid_mask], y[valid_mask])

    def create_model(self, degree=4):

        """
        Creates a polynomial regression pipeline with imputation.
    
        Args:
            degree (int): Degree of polynomial features (default=4).
            
        Returns:
            sklearn.pipeline.Pipeline: A pipeline with:
                - PolynomialFeatures
                - SimpleImputer (mean strategy)
                - LinearRegression
        """

        return make_pipeline(
            PolynomialFeatures(degree=degree, include_bias=False),
            SimpleImputer(strategy='mean'),
            LinearRegression()
        )
    
    def train_model(self, data, target, features=None):

        """
        Trains a model with the given data using the randomforestregressor

        Args:
            data(pd.DataFrame): Combined weather and air quality data
            target (str): Target variable name 
            features (list): List of feature column names (default=None utilises a default list)

        Returns:
            sklearn.ensemble.RandomForestRegressor: Trained model.
        """

        # Set default features if None
        potential_features = [
            'temperature (C)', 'wind_speed (m/s)', 
            'precipitation (mm)', 'DayOfYear_sin',
            'DayOfYear_cos', 'Weekend', 'Season'
        ]

        # Only calculate mutual info if features is None
        if features is None:
            self.logger.info("No features provided. Using mutual information for selection.")

            # Filter to only available features
            available_features = [f for f in potential_features if f in data.columns]
            
            # Calculate mutual information
            mi = mutual_info_regression(
                data[available_features].fillna(data[available_features].median()),
                data[target].fillna(data[target].median())
            )
            # Get top 5 features
            features = [f for _, f in sorted(zip(mi, available_features), reverse=True)][:5]
            self.logger.info(f"Selected features for {target}: {features}")

        # Input validation
        if target not in data.columns:
            raise ValueError(f"Target column '{target}' not found in data")
        missing_features = [f for f in features if f not in data.columns]
        if missing_features:
            raise ValueError(f"Missing features: {missing_features}")
        valid_data = data[data[target].notna()]
        if len(valid_data) == 0:
            raise ValueError(f"No valid rows remaining for target '{target}'")

        # Use of Train Random Forest model was suggested by AI for a better prediction (Deepseek)
        # Model configuration
        if target in ('NO2', 'NO'):
            model = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_leaf=3,
                max_features=0.5,
                random_state=42,
                n_jobs=-1
            )
        else:
            model = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_leaf=2,
                max_features=0.8,
                random_state=42,
                n_jobs=-1
            )

        model.fit(valid_data[features], valid_data[target])

        return model

    def predict_future(self, model, last_date, days_to_predict):

        """
        Generate future predictions using a simplistic prediction method with a trained model

        Args:
            model: Trained regression model
            last_date: Last date of historical data
            days_to_predict: Number of days to predict

        Returns:
            pd.Dataframe: DataFrame with columns:
                - 'Date': Future dates.
                - 'DayOfYear': Day of the year (1-365).
                - 'Prediction': Model predictions.
        """

        # Input validation
        try:
            last_date = pd.to_datetime(last_date)  # Convert if not already datetime
        except ValueError:
            raise ValueError("last_date must be a parsable date")
        
        if not isinstance(days_to_predict, int) or days_to_predict <= 0:
            raise ValueError("days_to_predict must be a positive integer")
        
        # Check model is fitted
        if not hasattr(model, 'predict'):
            raise ValueError("Provided model must have a predict method")
        
        future_dates = pd.date_range(
            start=last_date + timedelta(days=1),
            periods=days_to_predict
        )

        future_dates = [last_date + timedelta(days=i) for i in range(1, days_to_predict + 1)]
        day_of_year = [d.dayofyear for d in future_dates]
        future_df = pd.DataFrame({'Date': future_dates, 'DayOfYear': day_of_year})
        future_df['Prediction'] = model.predict(future_df[['DayOfYear']])

        return future_df

    def evaluate_model(self, model, test_data, target, features=None, train_data=None):

        """
        Evaluaes the model against the test data
        
        Args:
            model: Trained model
            test_data (pd.DataFrame): Combined weather and air quality data
            target (str): Target variable name 
            features (list): List of feature column names
            train_data: Training data used for median imputation 
        """

        if features is None:
            features = getattr(model, 'feature_names_in_', None)
            if features is None:
                raise ValueError("No features provided and model has no feature_names_in_ attribute")

        # Handle missing features by imputing with training median
        missing_features = set(features) - set(test_data.columns)
        if missing_features:
            self.logger.warning(f"Imputing missing features with training median: {missing_features}")
            if train_data is None:
                raise ValueError(f"Missing features {missing_features} and no train_data provided for imputation")
            test_data = test_data.copy()
            for f in missing_features:
                test_data[f] = train_data[f].median()

        # Input validation
        if target not in test_data.columns:
            raise ValueError(f"Target column '{target}' not found in data")
        missing_features = [f for f in features if f not in test_data.columns]
        if missing_features:
            raise ValueError(f"Missing features: {missing_features}")

        valid_mask = test_data[target].notna() & test_data[features].notna().all(axis=1)
        valid_data = test_data.loc[valid_mask].copy()

        if len(valid_data) == 0:
            self.logger.warning(f"No valid test rows for target '{target}'")
            return None, None, None

        predictions = model.predict(valid_data[features])
        mse = mean_squared_error(valid_data[target], predictions)
        r2 = r2_score(valid_data[target], predictions)

        return predictions, valid_data, mse, r2