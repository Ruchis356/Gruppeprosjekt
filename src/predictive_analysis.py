import pandas as pd
import numpy as np
from tqdm import tqdm  
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from datetime import timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import mutual_info_regression
import logging # The use of logging was suggested by AI (DeepSeek)
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)


class WeatherAnalyser:
    ''' A class for analysing and predicting weather and pollutions using historical data and the weather forecast.'''

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def load_weather_data(self, df_weather, weather_vars, show_info=False):
        """Load only weather data without merging with pollutants"""
        
        # Input validation
        if 'Date' not in df_weather.columns:
            raise ValueError("df_weather must contain a 'Date' column")
        
        # Filter available columns
        available_weather = [col for col in weather_vars if col in df_weather.columns]
        df = df_weather[['Date'] + available_weather].copy()
        
        # Add temporal features
        df['DayOfYear'] = df['Date'].dt.dayofyear
        df['DayOfYear_sin'] = np.sin(2 * np.pi * df['DayOfYear']/365)
        df['DayOfYear_cos'] = np.cos(2 * np.pi * df['DayOfYear']/365)
        
        return df

    def load_and_merge_data(self, df_weather, df_quality, weather_vars, pollutant_vars, show_info, mode='train'):
        
        """
        Merges dataframes.
        
        Args:
            df_weather: DataFrame with columns ['Date', 'temperature (C)', ...]
            df_quality: DataFrame with columns ['Date', 'PM10', ...]
            weather_vars: The columns to be found in df_weather
            pollutant_vars: The columns to be found in df_quality
            mode: 'train' or 'test' (to handle column name suffixes)

        Returns:
            Merged DataFrame with cleaned data and DayOfYear column
        """

        # Input validation

        if 'Date' not in df_weather.columns:
            raise ValueError("df_weather must contain a 'Date' column")
        if 'Date' not in df_quality.columns:
            raise ValueError("df_quality must contain a 'Date' column")

        # The following block of code was generated by AI
            # Purpose: graceful handling of missing columns in the dataframes
            # AI Tool: DeepSeek

        # Check available columns in both datasets and log any missing
        available_weather = [col for col in weather_vars if col in df_weather.columns]
        available_pollutants = [col for col in pollutant_vars if col in df_quality.columns]
        missing_weather = set(weather_vars) - set(available_weather)
        missing_pollutants = set(pollutant_vars) - set(available_pollutants)

        # Handle merging of test data
        if mode == 'test':
            df_quality = df_quality.rename(columns={c: c.replace('_test', '') for c in df_quality.columns})
        
        # Ensure we have at least one column from each dataset
        if not available_weather:
            raise ValueError("No weather columns available - check input data")
        if not available_pollutants:
            self.logger.warning("No pollutant columns available - proceeding with weather data only")

        # Merge with available columns
        merged = pd.merge(
            df_weather[['Date'] + available_weather],
            df_quality[['Date'] + available_pollutants],
            on='Date',
            how='inner'
        )
        
        # Add day of year 
        merged['DayOfYear'] = merged['Date'].dt.dayofyear
        
        if show_info is True:

            if missing_weather:
                self.logger.info(f"Skipping missing weather columns: {missing_weather}")
            if missing_pollutants:
                self.logger.info(f"Skipping missing pollutant columns: {missing_pollutants}")

            # The following block of code was generated by AI then refined by developers
                # Purpose: To provide detailed information about the model and its limitations
                # AI Tool: DeepSeek

            self.logger.info(
                f"\nMerged data shape: {merged.shape}, "
                f"\nAvailable weather: {available_weather}, "
                f"\nAvailable pollutants: {available_pollutants}"
            )

        # --- Seasonal Features ---
        merged['DayOfYear_sin'] = np.sin(2 * np.pi * merged['DayOfYear']/365)
        merged['DayOfYear_cos'] = np.cos(2 * np.pi * merged['DayOfYear']/365)
        merged['Weekend'] = merged['Date'].dt.weekday >= 5  # Saturday/Sunday
        merged['Season'] = merged['Date'].dt.month % 12 // 3 + 1  # 1-4
        merged['weekend_effect'] = merged['Weekend'] * merged['DayOfYear_sin']
        merged['Date'] = pd.to_datetime(merged['Date'])
        merged = merged.sort_values('Date') 
                
        # The following block of code was generated by AI
            # Purpose: Utilising interactions between predictive factors in modelling
            # AI Tool: DeepSeek
                             
        # --- Interaction Terms ---                
        if 'temperature (C)' in weather_vars and 'wind_speed (m/s)' in weather_vars:
            merged['temp_wind_interaction'] = merged['temperature (C)'] * merged['wind_speed (m/s)']
            
        if 'temperature (C)' in weather_vars and 'precipitation (mm)' in weather_vars:
            merged['temp_precip_interaction'] = merged['temperature (C)'] * merged['precipitation (mm)']

        if 'precipitation (mm)' in merged.columns and 'wind_speed (m/s)' in merged.columns:
            merged['precip_wind_interaction'] = merged['precipitation (mm)'] * merged['wind_speed (m/s)']

        # The following 6 blocks of code were created by developers and expanded by AI
            # Purpose: Creating multiple columns of features for the model, without manually typing them all out
            # AI Tool: DeepSeek

        interactions = []
        # Create all possible interaction terms
        for i, var1 in enumerate(weather_vars):
            for var2 in weather_vars[i+1:]:
                col_name = f"{var1.split()[0]}_{var2.split()[0]}_interaction"
                merged[col_name] = merged[var1] * merged[var2]
                interactions.append(col_name)

        # --- Rolling features (weather) ---
        for var in weather_vars:
            if var in merged.columns:
                base_name = var.split()[0]  
                merged[f'rolling_{base_name}_7'] = merged[var].rolling(7, min_periods=1).mean()

        for lag in [1, 2, 3]: 
            for weather_var in ['temperature (C)', 'wind_speed (m/s)', 'precipitation (mm)']:
                if weather_var in merged.columns:
                    merged[f'{weather_var}_lag_{lag}'] = merged[weather_var].shift(lag)

        # --- Rolling features (pollutants) ---
        for lag in [1, 3, 7, 30]:  # 1-day, 3-day, weekly, monthly lags
            for pollutant in pollutant_vars:
                if pollutant in merged.columns:
                    merged[f'{pollutant}_lag_{lag}'] = merged[pollutant].shift(lag)

        for pollutant in pollutant_vars:
            if pollutant in merged.columns:
                merged[f'rolling_{pollutant}_7'] = merged[pollutant].rolling(7, min_periods=1).mean()
                merged[f'{pollutant}_lag_14'] = merged[pollutant].shift(14)

        # --- Interpolation ---
        for pollutant in pollutant_vars:
            if pollutant in merged:
                na_count_before = merged[pollutant].isna().sum()
                merged[pollutant] = merged[pollutant].interpolate(limit=3)
                na_count_after = merged[pollutant].isna().sum()
                if na_count_after < na_count_before and show_info is True:
                    self.logger.info(f"Interpolated {na_count_before - na_count_after} missing values in {pollutant}")

        if show_info:
            self.logger.info(' \n')

        # The following block of code was generated by AI
            # Purpose: Suggested features for the models that might help improve the R2 score (it did not)
            # AI Tool: DeepSeek

        merged['spike_indicator'] = (merged['PM10_lag_1'] < 20) & (merged['wind_speed (m/s)'] < 3)
        merged['wind_speed_change'] = merged['wind_speed (m/s)'] - merged['wind_speed (m/s)_lag_1']

        merged = merged.dropna(subset=weather_vars)

        return merged
    
    def safe_fit(self, model, X, y):
        """
        Safely fits a model by dropping rows where the target (y) has NaN values.
        
        Args:
            model: A scikit-learn compatible model with a `fit` method.
            X (pd.DataFrame): Feature matrix.
            y (pd.Series): Target variable.
            
        Returns:
            The fitted model.

        Example:
            >>> model = LinearRegression()
            >>> safe_fit(model, X_train, y_train)  # Drops rows where y_train is NaN
        """

        # The following block of code was generated by AI
            # Purpose: Dealin with NaN values that were causing trouble in a more efficient manner
            # AI Tool: DeepSeek
    
        valid_mask = y.notna()
        return model.fit(X[valid_mask], y[valid_mask])

    def create_model(self, degree=4):

        """
        Creates a polynomial regression pipeline with imputation.
    
        Args:
            degree (int): Degree of polynomial features (default=4).
            
        Returns:
            sklearn.pipeline.Pipeline: A pipeline with:
                - PolynomialFeatures
                - SimpleImputer (mean strategy)
                - LinearRegression
        """

        return make_pipeline(
            PolynomialFeatures(degree=degree, include_bias=False),
            SimpleImputer(strategy='mean'),
            LinearRegression()
        )
    
    def train_model(self, data, target, features=None):

        """
        Trains a model with the given data using the randomforestregressor

        Args:
            data(pd.DataFrame): Combined weather and air quality data
            target (str): Target variable name 
            features (list): List of feature column names (default=None utilises a default list)

        Returns:
            sklearn.ensemble.RandomForestRegressor: Trained model.
        """

        # The following 2 blocks of code was developed with assistance from AI
            # Purpose: Creating a functional method of dealing with features if none are provided
            # AI Tool: DeepSeek

        # Set default features if None
        potential_features = [
            'temperature (C)', 'wind_speed (m/s)', 
            'precipitation (mm)', 'DayOfYear_sin',
            'DayOfYear_cos', 'Weekend', 'Season'
        ]

        # Only calculate mutual info if features is None
        if features is None:
            self.logger.info("No features provided. Using mutual information for selection.")

            # Filter to only available features
            available_features = [f for f in potential_features if f in data.columns]
            
            # Calculate mutual information
            mi = mutual_info_regression(
                data[available_features].fillna(data[available_features].median()),
                data[target].fillna(data[target].median())
            )
            # Get top 5 features
            features = [f for _, f in sorted(zip(mi, available_features), reverse=True)][:5]
            self.logger.info(f"Selected features for {target}: {features}")

        # Input validation
        if target not in data.columns:
            raise ValueError(f"Target column '{target}' not found in data")
        missing_features = [f for f in features if f not in data.columns]
        if missing_features:
            raise ValueError(f"Missing features: {missing_features}")
        valid_data = data[data[target].notna()]
        if len(valid_data) == 0:
            raise ValueError(f"No valid rows remaining for target '{target}'")

        # Use of Train Random Forest model was suggested by AI for a better prediction (Deepseek)

        # Model configuration
        if target in ('NO2', 'NO'):
            model = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_leaf=3,
                max_features=0.5,
                random_state=42,
                n_jobs=-1
            )
        else:
            model = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_leaf=2,
                max_features=0.8,
                random_state=42,
                n_jobs=-1
            )

        model.fit(valid_data[features], valid_data[target])

        return model

    def predict_future(self, model, last_date, days_to_predict):

        """
        Generate future predictions using a simplistic prediction method with a trained model

        Args:
            model: Trained regression model
            last_date: Last date of historical data
            days_to_predict: Number of days to predict

        Returns: 
            pd.Dataframe: DataFrame with columns:
                - 'Date': Future dates.
                - 'DayOfYear': Day of the year (1-365).
                - 'Prediction': Model predictions.
        """

        # Input validation    
        try:
            last_date = pd.to_datetime(last_date) 
        except ValueError:
            raise ValueError("last_date must be a parsable date")
        
        if not isinstance(days_to_predict, int) or days_to_predict <= 0:
            raise ValueError("days_to_predict must be a positive integer")
        
        # Check model is fitted
        if not hasattr(model, 'predict'):
            raise ValueError("Provided model must have a predict method")

        # Create continuous date range starting day after last_date
        future_dates = pd.date_range(
            start=last_date + pd.Timedelta(days=1),
            periods=days_to_predict,
            freq='D'
        )
        
        # Create DataFrame with DayOfYear
        future_df = pd.DataFrame({
            'Date': future_dates,
            'DayOfYear': future_dates.dayofyear
        })

        # Model validation and prediction
        if not hasattr(model, 'predict'):
            raise ValueError("Model must have predict method")
        
        # Maintain your existing feature warning
        if hasattr(model, 'feature_names_in_') and 'DayOfYear' not in model.feature_names_in_:
            self.logger.warning(
                "Model expects features: %s. Predictions may be unreliable "
                "as only 'DayOfYear' is provided.", 
                model.feature_names_in_
            )
        
        future_df['Prediction'] = model.predict(future_df[['DayOfYear']])

        return future_df

    def evaluate_model(self, model, test_data, target, features=None, train_data=None):

        """
        Evaluaes the model against the test data
        
        Args:
            model: Trained model
            test_data (pd.DataFrame): Combined weather and air quality data
            target (str): Target variable name 
            features (list): List of feature column names
            train_data: Training data used for median imputation 

        Returns:
            Tuple: (predictions, valid_data, mse, r2)  
                - predictions (pd.DataFrame): Array of model predictions
                - valid_data (pd.DataFrame): Filtered(NaN) DataFrame used for evaluation 
                - mse (float):  Mean squared error for the model
                - r2 (float): RÂ² score for the model
            Returns tuple(None, None, None, None) if no valid rows remain after NaN filtering

        Note: The returns were generated by AI and edited by developers
        """

        # The following block of code was developed with assistance from AI
            # Purpose: Deciding which edge cases to run through input validation
            # AI Tool: DeepSeek

        # Input validation
        if test_data.empty:
            raise ValueError("test_data is empty")
        if features is None:
            features = getattr(model, 'feature_names_in_', None)
            if features is None:
                raise ValueError("No features provided and model has no feature_names_in_ attribute")
        if target not in test_data.columns:
            raise ValueError(f"Target column '{target}' not found in data")
        missing_features = [f for f in features if f not in test_data.columns]
        if missing_features:
            raise ValueError(f"Missing features: {missing_features}")

        # Create explicit copies with .copy() to avoid SettingWithCopyWarning suggested AI (Deepseek)

        # Handle missing features by imputing with training median
        missing_features = set(features) - set(test_data.columns)
        if missing_features:
            self.logger.warning(f"Imputing missing features with training median: {missing_features}")
            if train_data is None:
                raise ValueError(f"Missing features {missing_features} and no train_data provided for imputation")
            test_data = test_data.copy()
            for f in missing_features:
                test_data[f] = train_data[f].median()

        valid_mask = test_data[target].notna() & test_data[features].notna().all(axis=1)
        valid_data = test_data.loc[valid_mask].copy()

        if len(valid_data) == 0:
            self.logger.warning(f"No valid test rows for target '{target}'")
            return None, None, None, None

        predictions = model.predict(valid_data[features])
        mse = mean_squared_error(valid_data[target], predictions)
        r2 = r2_score(valid_data[target], predictions)

        return predictions, valid_data, mse, r2
    
    def forecast_pollutants_with_lags(self, weather_forecast, models_dict, features_dict, 
                                    historical_data, n_days=7):
        """
        Forecast pollutant levels with proper handling of lag features
        
        Args:
            weather_forecast: DataFrame with future weather predictions
            models_dict: Dictionary of trained models
            features_dict: Dictionary of feature lists
            historical_data: DataFrame with recent historical data (for initial lags)
            n_days: Number of days to forecast
            
        Returns:
            Dictionary of DataFrames with forecasts for each pollutant
        """
        
        forecasts = {}
        required_weather = ['temperature (C)', 'precipitation (mm)', 'wind_speed (m/s)']
        
        # Prepare working copy with historical data for lags
        full_df = pd.concat([historical_data, weather_forecast], axis=0).reset_index(drop=True)
        full_df = full_df.sort_values('Date').reset_index(drop=True)
        
        # For each pollutant, generate recursive forecasts
        for pollutant, model in tqdm(models_dict.items(), desc="Forecasting pollutants"):
            features = features_dict[pollutant]
            result = pd.DataFrame()
            
            # Initialize with historical data
            working_df = full_df.copy()
            
            # Get the max lag we need (e.g., lag_14 means we need 14 days history)
            max_lag = max([int(f.split('_')[-1]) for f in features 
                        if f.startswith(f"{pollutant}_lag_") and f.split('_')[-1].isdigit()], 
                        default=0)
            
            # Only forecast days where we have weather data
            forecast_start_idx = len(historical_data)
            forecast_end_idx = min(forecast_start_idx + n_days, len(working_df))
            
            # Recursive prediction day by day
            for i in range(forecast_start_idx, forecast_end_idx):
                # Prepare features for this day
                current_features = {}
                
                for feature in features:
                    # Handle lag features
                    if feature.startswith(f"{pollutant}_lag_"):
                        lag_days = int(feature.split('_')[-1])
                        if i - lag_days >= 0:
                            current_features[feature] = working_df.at[i - lag_days, pollutant]
                    
                    # Handle rolling features
                    elif feature.startswith(f"rolling_{pollutant}_"):
                        window = int(feature.split('_')[-1])
                        lookback = working_df.iloc[max(0, i-window):i][pollutant]
                        if len(lookback) > 0:
                            current_features[feature] = lookback.mean()
                    
                    # Handle weather features
                    elif feature in required_weather:
                        current_features[feature] = working_df.at[i, feature]
                    
                    # Handle other features (interactions, etc.)
                    else:
                        try:
                            current_features[feature] = working_df.at[i, feature]
                        except KeyError:
                            pass  # Will be handled by model
                
                # Convert to DataFrame for prediction
                feature_df = pd.DataFrame([current_features])
                
                # Make prediction (handle missing features)
                available_features = [f for f in features if f in feature_df.columns]
                if not available_features:
                    raise ValueError(f"No available features for {pollutant} on day {i}")
                
                prediction = model.predict(feature_df[available_features])[0]
                
                # Store prediction and update working DataFrame
                working_df.at[i, pollutant] = prediction
                date = working_df.at[i, 'Date']
                
                # Store results
                result = pd.concat([result, pd.DataFrame({
                    'Date': [date],
                    f'{pollutant}_forecast': [prediction]
                })])
            
            forecasts[pollutant] = result.sort_values('Date')
        
        return forecasts